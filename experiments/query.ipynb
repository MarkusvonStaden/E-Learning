{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative KI kann die Selbstständigkeit einer Abschlussarbeit in Frage stellen, wenn die KI-generierten Textpassagen quantitativ oder qualitativ die Arbeit prägen. Es wird empfohlen, Prüfungsformen zu wählen, die nicht oder nur schwer durch KI beeinflusst werden können, um die Selbstständigkeit der Prüflinge zu gewährleisten. Der Einsatz von generativer KI muss bewusst und transparent erfolgen, um sicherzustellen, dass eine eigenständige Kernleistung erbracht wird.\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "vector_store = Chroma(embedding_function=embedding_function, persist_directory='chroma')\n",
    "\n",
    "results = vector_store.similarity_search_with_relevance_scores(query, 5)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(\"I'm sorry, I don't understand.\")\n",
    "else:\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "    chat_model = ChatOpenAI()\n",
    "    response = chat_model.predict(prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
